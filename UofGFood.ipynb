{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from slugify import slugify\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "base_dir = \"UofGFood\"\n",
    "tweets_dir = \"UofGFood/json-tweets\"\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "if not os.path.exists(tweets_dir):\n",
    "    os.makedirs(tweets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets with potential links 838\n"
     ]
    }
   ],
   "source": [
    "def date_ms(timestamp):\n",
    "    return datetime.datetime.fromtimestamp(timestamp)\n",
    "\n",
    "tweets_with_links = []\n",
    "for file in glob.glob(join(tweets_dir,\"*.json\")):\n",
    "    tweets = None\n",
    "    with open(file, \"r\") as f:\n",
    "        tweets = json.load(f)\n",
    "    for t in tweets:\n",
    "        beautiful_soup = BeautifulSoup(t['rawHtml'], \"lxml\")\n",
    "        anchors = beautiful_soup.findAll('a')\n",
    "        if anchors is not None:\n",
    "            _as = []\n",
    "            for a in anchors:\n",
    "                expanded_url = a.get('data-expanded-url')\n",
    "                if expanded_url is not None:\n",
    "                    _as.append(expanded_url)\n",
    "            if len(_as) > 0:\n",
    "                item = {\n",
    "                    'tweetId': t['tweetId'],\n",
    "                    'rawHtml': t['rawHtml'],\n",
    "                    'timestamp':date_ms(int(t['timestamp'])),\n",
    "                    'links': _as\n",
    "                }\n",
    "                tweets_with_links.append(item)\n",
    "tweets_with_links = sorted(tweets_with_links, key=lambda item: item[\"timestamp\"])\n",
    "print(\"Tweets with potential links\",len(tweets_with_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menu 3 66\n",
      "Err link http://weeks.Feedback\n",
      "Menu 4 40\n",
      "Menu 5 44\n",
      "Menu 6 51\n",
      "Err link http://Square.Coffee\n",
      "Menu 7 51\n",
      "Menu 8 12\n"
     ]
    }
   ],
   "source": [
    "t_l = [\n",
    "    tweets_with_links[0:100],\n",
    "    tweets_with_links[100:200],\n",
    "    tweets_with_links[200:300],\n",
    "    tweets_with_links[300:400],\n",
    "    tweets_with_links[400:500],\n",
    "    tweets_with_links[500:600],\n",
    "    tweets_with_links[600:700],\n",
    "    tweets_with_links[700:800],\n",
    "    tweets_with_links[800:900],\n",
    "#    tweets_with_links[900:1000],\n",
    "#    tweets_with_links[1000:1200]\n",
    "]\n",
    "for i in range(3,len(t_l)):\n",
    "    tll = t_l[i]\n",
    "    menu = []\n",
    "    for t in tll:\n",
    "        for l in t[\"links\"]:\n",
    "            soup = None\n",
    "            try:\n",
    "                soup = BeautifulSoup(requests.get(l).text, \"lxml\")\n",
    "            except:\n",
    "                print(\"Err link\", l)\n",
    "                continue\n",
    "            categoryTrail = soup.find('p', {'class':'categoryTrail'})\n",
    "            if categoryTrail is not None and categoryTrail.text == 'Food':\n",
    "                title = soup.find('h1').text.strip()\n",
    "                authorDate = soup.find('p', {'class':'authorDate'})\n",
    "                author = authorDate.find('a').text.strip()\n",
    "                date = authorDate.find('span').text.strip()\n",
    "                articleText = soup.find('div', {'class':'articleText'})\n",
    "\n",
    "                if articleText is None:\n",
    "                    continue\n",
    "                _h3s = []\n",
    "                textArticle = None\n",
    "                h3s = articleText.findAll('h3')\n",
    "                if len(h3s) > 0:\n",
    "                    for h3 in h3s:\n",
    "                        _h3s.append(h3.text.strip())\n",
    "                else:\n",
    "                    textArticle = articleText.text\n",
    "\n",
    "                menu_entry = {\n",
    "                    'tweetId': t[\"tweetId\"],\n",
    "                    'tweet_date': t[\"timestamp\"].strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'title': title,\n",
    "                    'date': date,\n",
    "                    'author': author,\n",
    "                    'entries': _h3s,\n",
    "                    'text': textArticle\n",
    "                }\n",
    "                \n",
    "                menu.append(menu_entry)\n",
    "    with open(join(\"UofGFood\", \"menu-\"+ str(i) + \".json\"), \"w\") as ww:\n",
    "        json.dump(menu, ww, indent=4)\n",
    "    print(\"Menu\", i, len(menu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menu 100\n",
      "Menu 100\n",
      "Menu 100\n",
      "Menu 100\n",
      "Menu 100\n",
      "Menu 100\n",
      "Menu 100\n",
      "Menu 100\n",
      "Menu 38\n",
      "Menu 0\n",
      "Menu 0\n"
     ]
    }
   ],
   "source": [
    "t_l = [\n",
    "    tweets_with_links[0:100],\n",
    "    tweets_with_links[100:200],\n",
    "    tweets_with_links[200:300],\n",
    "    tweets_with_links[300:400],\n",
    "    tweets_with_links[400:500],\n",
    "    tweets_with_links[500:600],\n",
    "    tweets_with_links[600:700],\n",
    "    tweets_with_links[700:800],\n",
    "    tweets_with_links[800:900],\n",
    "    tweets_with_links[900:1000],\n",
    "    tweets_with_links[1000:1200]\n",
    "]\n",
    "for ll in t_l:\n",
    "    print(\"Menu\", len(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_date = re.compile('\\w+\\s[0-9]+[a-z]{0,2}\\s\\w+\\s[0-9]{4}')\n",
    "\n",
    "def process_menu(menu):\n",
    "    clean = []\n",
    "    search_date = None\n",
    "    for entry in menu[\"entries\"]:\n",
    "        cl = ' '.join([x.strip() for x in entry.split('\\u00a0') if x != ''])\n",
    "        search_date = regex_date.search(cl)\n",
    "        if search_date:\n",
    "            menu[\"entry_date\"] = cl\n",
    "        else:\n",
    "            clean.append(cl)\n",
    "    if \"text\" in menu and menu[\"text\"] is not None:\n",
    "        menu[\"text\"] = ' '.join([x.strip() for x in menu[\"text\"].split('\\u00a0') if x != ''])\n",
    "        \n",
    "    \n",
    "    if search_date:\n",
    "        print(clean[0])\n",
    "    \n",
    "    menu[\"entries\"] = clean\n",
    "    \n",
    "for file in glob.glob(\"UofGFood/*.json\"):\n",
    "    menus = None\n",
    "    with open(file, \"r\") as menu_json:\n",
    "        menus = json.load(menu_json)\n",
    "\n",
    "    for menu in menus:\n",
    "        process_menu(menu)\n",
    "    with open(file, \"w\") as ww:\n",
    "        json.dump(menus, ww, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info_regex = re.compile('([a-zA-Z0-9\\s\\'&,]+)\\s([A-Z]+)$')\n",
    "key_regex = re.compile('([A-Z]+\\s=\\s\\w+)')\n",
    "\n",
    "def process_entries(menu):\n",
    "    new_entries = []\n",
    "    for entry in menu[\"entries\"]:\n",
    "        if entry == '':\n",
    "            continue\n",
    "        info_search = info_regex.search(entry)\n",
    "        key_search = key_regex.findall(entry)\n",
    "        new_entry = {}\n",
    "        if info_search:\n",
    "            new_entry[\"dish\"] =  info_search.group(1).strip()\n",
    "            new_entry[\"info\"] = info_search.group(2).strip()\n",
    "        elif key_search:\n",
    "            keys = {}\n",
    "            for key in key_search:\n",
    "                ky = key.split('=')\n",
    "                keys[ky[0].strip()] = ky[1].strip()\n",
    "            menu[\"key\"] = keys\n",
    "            continue\n",
    "        else:\n",
    "            new_entry[\"dish\"] = entry\n",
    "        new_entries.append(new_entry)\n",
    "    menu[\"entries\"] = new_entries\n",
    "        \n",
    "\n",
    "for file in glob.glob(\"UofGFood/*.json\"):\n",
    "    \n",
    "    menus = None\n",
    "    with open(file, \"r\") as menu_json:\n",
    "        menus = json.load(menu_json)\n",
    "\n",
    "    for menu in menus:\n",
    "        process_entries(menu)\n",
    "        \n",
    "    with open(file, \"w\") as ww:\n",
    "        json.dump(menus, ww, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_frasers = []\n",
    "others = []\n",
    "\n",
    "i = 0\n",
    "for file in glob.glob(\"UofGFood/*.json\"):\n",
    "    menus = None\n",
    "    with open(file, \"r\") as menu_json:\n",
    "        menus = json.load(menu_json)\n",
    "        for menu in menus:\n",
    "            title = menu['title'].lower()\n",
    "            if \"food for thought\" in title  or \"fraser\" in title:\n",
    "                only_frasers.append(menu)\n",
    "            else:\n",
    "                others.append(menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create an instance of the NLTK Porter Stemmer discussed in lecture to use.\n",
    "stemmer = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Modify the code below to 1) not append noisy words: words in stop_words or words that are too long or too short.\n",
    "# Question: What are good values for too long or too short? Why?\n",
    "def canonicalize(string):\n",
    "  normalized_tokens = list()\n",
    "  tokens = tokenizer.tokenize(string)\n",
    "  for t in tokens:\n",
    "    normalized = t.lower()\n",
    "    if normalized in stop_words:\n",
    "        continue\n",
    "    if 1 < len(normalized) < 25:\n",
    "        normalized_tokens.append(normalized)\n",
    "  return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'parse_dates'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-21a4d2160227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#all_tokens.extend(canonicalize(entry['dish']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmenu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#word_dist = nltk.FreqDist(all_tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'parse_dates'"
     ]
    }
   ],
   "source": [
    "frame = []\n",
    "\n",
    "# Put the entire array of tokens into a NLTK Frequency Distribution class.\n",
    "for menu in only_frasers:\n",
    "    entries = []\n",
    "    for entry in menu['entries']:\n",
    "        entries.extend(canonicalize(entry['dish']))\n",
    "        #all_tokens.extend(canonicalize(entry['dish']))\n",
    "    frame.append([menu['tweet_date'], entries])\n",
    "f = pd.DataFrame(frame, columns=['date', 'words'], parse_dates='date')\n",
    "f.head()\n",
    "#word_dist = nltk.FreqDist(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 317 different words\n",
      "There are 9056 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rice', 317),\n",
       " ('chicken', 307),\n",
       " ('potato', 226),\n",
       " ('baked', 196),\n",
       " ('veg', 186),\n",
       " ('soup', 183),\n",
       " ('chips', 171),\n",
       " ('fry', 170),\n",
       " ('wok', 146),\n",
       " ('stir', 144),\n",
       " ('station', 139),\n",
       " ('bread', 138),\n",
       " ('cheese', 129),\n",
       " ('roast', 126),\n",
       " ('beef', 118),\n",
       " ('vegetable', 112),\n",
       " ('tomato', 107),\n",
       " ('serving', 101),\n",
       " ('bbq', 97),\n",
       " ('pork', 91),\n",
       " ('pasta', 86),\n",
       " ('vegetables', 83),\n",
       " ('sauce', 83),\n",
       " ('pea', 81),\n",
       " ('bean', 81),\n",
       " ('meal', 81),\n",
       " ('brown', 80),\n",
       " ('soft', 78),\n",
       " ('staff', 78),\n",
       " ('potatoes', 75)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"There are\", word_dist.B(),\"different words\")\n",
    "print(\"There are\", word_dist.N(),\"words\")\n",
    "word_dist.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
