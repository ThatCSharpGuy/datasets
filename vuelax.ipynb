{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from slugify import slugify\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "base_dir = \"vuelax\"\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {'enero':1, 'febrero':2, 'marzo':3,\n",
    "          'abril':4, 'mayo':5, 'junio':6,\n",
    "          'julio':7,'agosto':8, 'septiembre':9,\n",
    "          'octubre':10, 'noviembre':11, 'diciembre':12}\n",
    "\n",
    "date_regex = re.compile('(\\w+) ([0-9]+), ([0-9]{4})')\n",
    "\n",
    "def date_converter(date):\n",
    "    found = date_regex.search(date)\n",
    "    if found:\n",
    "        return datetime.datetime(year=int(found.group(3)), month=months[found.group(1)], day=int(found.group(2)))\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url = \"http://www.vuelax.com/category/%s/page/%d/\"\n",
    "\n",
    "categories = ['oportunidades', 'uncategorized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dates = { }\n",
    "category_previous_frames = {}\n",
    "data = None\n",
    "last_date = None\n",
    "for category in categories:\n",
    "    original_file = join(base_dir, \"%s.csv\" % category)\n",
    "    if os.path.exists(original_file):\n",
    "        data = pd.read_csv(original_file, index_col=0, parse_dates=['date'], encoding='utf-8')\n",
    "        last_date = data.iloc[0]['date']\n",
    "        category_previous_frames[category] = data\n",
    "        category_dates[category] = last_date\n",
    "        print(\"Last date for \\\"%s\\\"\" % category, last_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    content = []\n",
    "    break_category = False\n",
    "    for page in range(1, 1000000):\n",
    "        url = page_url % (category, page)\n",
    "        op_page = requests.get(url)\n",
    "        if page % 10 == 0:\n",
    "            print(\"Requesting\", url)\n",
    "        if op_page.status_code != 200:\n",
    "            break\n",
    "        op_soup = BeautifulSoup(op_page.text, \"html.parser\")\n",
    "        main_ul = op_soup.find(\"ul\", {\"class\":\"penci-grid\"})\n",
    "        articles = main_ul.findAll(\"article\", {\"class\":\"item\"})\n",
    "        for article in articles:\n",
    "            grid_title = article.find(\"h2\", {\"class\":\"grid-title\"})\n",
    "            a = grid_title.find(\"a\")\n",
    "            grid_post_box_meta = article.find(\"div\", {\"class\":\"grid-post-box-meta\"})\n",
    "            date = date_converter(grid_post_box_meta.text.strip())\n",
    "            if date <= category_dates[category]:\n",
    "                print(\"I already have the date\", date, \"for the category \\\"%s\\\"\"%category)\n",
    "                break_category = True\n",
    "                break\n",
    "            content.append([a.text, a.get('href'), date])\n",
    "        if break_category:\n",
    "            break\n",
    "        \n",
    "    data =  pd.DataFrame(content, columns= [\"label\", \"url\", \"date\"])\n",
    "    \n",
    "    \n",
    "    if category in category_previous_frames:\n",
    "        data = pd.concat([category_previous_frames[category], data])\n",
    "    data.sort_values(by=['date', 'label'], ascending= False, inplace=True)\n",
    "    data.to_csv(join(base_dir, \"%s.csv\" % category), encoding='utf-8')\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for category in categories:\n",
    "    frame = pd.read_csv(join(base_dir, \"%s.csv\" % category), index_col=0, encoding='utf-8')\n",
    "    frames.append(frame)\n",
    "data = pd.concat(frames).reset_index(drop=True)\n",
    "\n",
    "print(len(data))\n",
    "not_duplicated = data[~data.duplicated('url')]\n",
    "print(len(not_duplicated))\n",
    "not_duplicated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple text processing\n",
    "(lots of regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_regex = re.compile('¡?([\\w0-9,\\s\\.]+)' + # Origin 0\n",
    "                            '\\s+[a|A]\\s+'+\n",
    "                            '([\\w0-9,\\s\\.]+)!?' + # Destination 1\n",
    "                            '\\s*[-|–|\"desde\"|\"DESDE\"]{0,1}\\s*' + \n",
    "                            '\\$([0-9\\.,]+)') # Price 2\n",
    "\n",
    "compact_location = re.compile('¡([\\w0-9,\\s\\.]+)' + # Origin 0\n",
    "                              '\\s+[a|A]\\s' + \n",
    "                              '([\\w0-9,\\s\\.]+)' + # Destination 1\n",
    "                              '\\s+[-|–|\"desde\"|\"DESDE\"]{0,1}\\s+'+\n",
    "                              '\\$([0-9\\.,]+)!' + # Price 2\n",
    "                              '\\s*\\(?([\\w\\s\\+]+)\\)?\\s*') # Note 3\n",
    "                                 \n",
    "location_regex_note = re.compile('¡?([\\w0-9,\\s\\.]+)' + # Origin 0\n",
    "                                 '\\s+[a|A]\\s' + \n",
    "                                 '([\\w0-9,\\s\\.]+)!?' + # Destination 1\n",
    "                                 '\\s*\\(?([\\w\\s\\+]+)\\)?\\s*' +  # Note 2\n",
    "                                 '[-|–|\"desde\"|\"DESDE\"]{0,1}\\s*'+\n",
    "                                 '\\$([0-9\\.,]+)') # Price 3\n",
    "\n",
    "def find_info(label):\n",
    "    \n",
    "    from_, to, price, note = None, None, None, None\n",
    "    find = compact_location.search(label)\n",
    "    if find: # compact\n",
    "        from_ = find.group(1)\n",
    "        to = find.group(2)\n",
    "        price = find.group(3).strip('.')\n",
    "    else:\n",
    "        find = location_regex.search(label)\n",
    "        if find: # without note\n",
    "            from_ = find.group(1)\n",
    "            to = find.group(2)\n",
    "            price = find.group(3).strip('.')\n",
    "        else:\n",
    "            find = location_regex_note.search(label)\n",
    "            if find:\n",
    "                from_ = find.group(1)\n",
    "                to = find.group(2)\n",
    "                note = find.group(3).strip()\n",
    "                price = find.group(4).strip('.')\n",
    "    return from_, to, price, note\n",
    "\"\"\"\n",
    "print(find_info(\"¡CDMX a Madrid! – $11,866. ¡Opciones disponibles en verano!\"))\n",
    "print(find_info(\"¡CDMX a Cancún! Vuelos + Hotel Todo Incluido – $5,258\"))\n",
    "print(find_info(\"CDMX y GDL a Busan, Corea. ¡Opciones de hospedaje desde 189 MXN la noche!\"))\n",
    "print(find_info(\"CUN a Bélgica – $7,807\"))\n",
    "print(find_info(\"¡CDMX, MTY, GDL, TIJ, Silao, CUL y más a Lima + Cusco – $7,125! Opción de hotel, 8 días y 7 noches por $2,685 por persona (hab doble)\"))\n",
    "print(find_info(\"¡CDMX a Bogotá + Cartagena $5,200! (Y por sólo $3,213 adicionales agrega 10 noches de hospedaje)\"))\n",
    "print(find_info(\"CDMX y muchas ciudades a Panamá – $4,566. ¡Opción con hospedaje y desayuno desde 330 MXN la noche!\"))ç\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_values = []\n",
    "non_clean_values = []\n",
    "\n",
    "for index, row in not_duplicated.iterrows():\n",
    "    label = row['label']\n",
    "    from_, to, price, note = find_info(label)\n",
    "    \n",
    "    if from_:\n",
    "        clean_values.append([from_, to, float(price.replace(\",\",\"\")),note, row[\"url\"], row[\"date\"], label])\n",
    "    else:\n",
    "        non_clean_values.append(row.values)\n",
    "\n",
    "\n",
    "clean = pd.DataFrame(clean_values, columns= [\"origin\", \"destination\", \"price\", \"note\", \"url\", \"date\", \"label\"])\n",
    "still_dirty_df = pd.DataFrame(non_clean_values, columns= [\"label\", \"url\", \"date\"])\n",
    "\n",
    "\n",
    "print(\"== Clean ==\")\n",
    "print(clean[pd.notna(clean.note)].iloc[:3][[\"note\", \"label\"]].values)\n",
    "print()\n",
    "print(\"== Dirty ==\")\n",
    "print(len(still_dirty_df.label.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize.regexp import regexp_tokenize\n",
    "tolist  = lambda origin: [t.strip()\n",
    "                          for t \n",
    "                          in regexp_tokenize(origin, r'[y,\\.\\?!\"]\\s+', gaps=True)]\n",
    "\n",
    "airports_df = pd.read_csv(join(base_dir, \"airports.csv\"), index_col=0)\n",
    "airports = dict(zip(airports_df.IATA, airports_df[\"Location served\"]))\n",
    "\n",
    "real_locations = {'CDMX': 'Ciudad de México', \n",
    "                  'CUN': 'Cancún', 'GDL': 'Guadalajara',\n",
    "                  'L.A.': 'Los Angeles', 'LA': 'Los Angeles',\n",
    "                  'NYC': 'New York City', \n",
    "                  'MTY': 'Monterrey', \n",
    "                  'PUE': 'Puebla', 'QRO': 'Querétaro',\n",
    "                  'SLP': 'San Luis Potosí',\n",
    "                  'TIJ': 'Tijuana', 'VER': 'Veracruz'\n",
    "                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_origins = []\n",
    "\n",
    "year = re.compile(\"\\.*[0-9]+\\.*\")\n",
    "has_numbers = lambda f: True if year.search(f) else False\n",
    "\n",
    "for index, row in clean.iterrows():\n",
    "    origins = tolist(row['origin'])\n",
    "    for origin in origins:\n",
    "        destination = row['destination'].strip()\n",
    "        if not has_numbers(origin): # filter out stuff like 23 cities more...\n",
    "            separate_origins.append([ \n",
    "                real_locations.get(origin.strip(), origin.strip()), \n",
    "                real_locations.get(destination, destination),\n",
    "                row['date'], row['price'],\n",
    "                row['note'], row['url']])\n",
    "        \n",
    "separa_origin_df = pd.DataFrame(separate_origins, columns=['origin', 'destination',\n",
    "                                                           'date', 'price', \n",
    "                                                           'note', 'url'])\n",
    "\n",
    "\n",
    "\n",
    "separa_origin_df.to_csv(join(base_dir, \"separate_origins.csv\"), encoding='utf-8')\n",
    "separa_origin_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_locations = sorted(set(list(separa_origin_df.origin.unique()) + list(separa_origin_df.destination.unique())))\n",
    "print(len(unique_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_locations = join(base_dir, \"location_data.json\")\n",
    "location_dic = {}\n",
    "if os.path.exists(json_locations):\n",
    "    with open(json_locations, \"r\") as s:\n",
    "        location_dic = json.load(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "parameters = {\n",
    "    'bounds':'',\n",
    "    'components':'',\n",
    "    'region': '',\n",
    "    'language':'en', \n",
    "    'key': ''\n",
    "}\n",
    "\n",
    "for l in unique_locations:\n",
    "    l = real_locations.get(l, l)\n",
    "    if l in location_dic:\n",
    "        continue\n",
    "    print(\"Reading %s\" % l)\n",
    "    parameters[\"address\"] = l\n",
    "    mapinfo = requests.get(\"https://maps.googleapis.com/maps/api/geocode/json\", parameters)\n",
    "    if mapinfo.status_code == 200:\n",
    "        location_dic[l] = json.loads(mapinfo.text)\n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(base_dir, \"location_data.json\"), \"w\") as s:\n",
    "    json.dump(location_dic, s, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_original_location(location):\n",
    "    if location in location_dic:\n",
    "        jj = location_dic[location]\n",
    "        if len(jj[\"results\"]) > 0:\n",
    "            return jj[\"results\"][0][\"formatted_address\"]\n",
    "    return location\n",
    "\n",
    "def get_original_country(location):\n",
    "    if location in location_dic:\n",
    "        jj = location_dic[location]\n",
    "        if len(jj[\"results\"]) > 0:\n",
    "            address_components = jj[\"results\"][0][\"address_components\"]\n",
    "            for component in address_components:\n",
    "                if \"country\" in component[\"types\"]:\n",
    "                    return component[\"long_name\"] + \" (\" + component[\"short_name\"] + \")\"\n",
    "    return None\n",
    "\n",
    "def get_original_administrative_area_level_1(location):\n",
    "    if location in location_dic:\n",
    "        jj = location_dic[location]\n",
    "        if len(jj[\"results\"]) > 0:\n",
    "            address_components = jj[\"results\"][0][\"address_components\"]\n",
    "            for component in address_components:\n",
    "                if \"administrative_area_level_1\" in component[\"types\"]:\n",
    "                    return component[\"long_name\"] + \" (\" + component[\"short_name\"] + \")\"\n",
    "    return None\n",
    "\n",
    "def get_lat(location):\n",
    "    if location in location_dic:\n",
    "        jj = location_dic[location]\n",
    "        if len(jj[\"results\"]) > 0:\n",
    "            return jj[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]\n",
    "    return np.nan\n",
    "\n",
    "def get_long(location):\n",
    "    if location in location_dic:\n",
    "        jj = location_dic[location]\n",
    "        if len(jj[\"results\"]) > 0:\n",
    "            return jj[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]\n",
    "    return np.nan\n",
    "\n",
    "separa_origin_df[\"clean_origin\"] = separa_origin_df.origin.apply(get_original_location)\n",
    "separa_origin_df[\"clean_destination\"] = separa_origin_df.destination.apply(get_original_location)\n",
    "\n",
    "separa_origin_df[\"country_origin\"] = separa_origin_df.origin.apply(get_original_country)\n",
    "separa_origin_df[\"country_destination\"] = separa_origin_df.destination.apply(get_original_country)\n",
    "\n",
    "separa_origin_df[\"area_level_1_origin\"] = separa_origin_df.origin.apply(get_original_administrative_area_level_1)\n",
    "separa_origin_df[\"area_level_1_destination\"] = separa_origin_df.destination.apply(get_original_administrative_area_level_1)\n",
    "\n",
    "separa_origin_df['origin_lat'] = separa_origin_df.origin.apply(get_lat)\n",
    "separa_origin_df['origin_long'] = separa_origin_df.origin.apply(get_long)\n",
    "separa_origin_df['destination_lat'] = separa_origin_df.destination.apply(get_lat)\n",
    "separa_origin_df['destination_long'] = separa_origin_df.destination.apply(get_long)\n",
    "\n",
    "separa_origin_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "separa_origin_df = separa_origin_df[['origin','country_origin','area_level_1_origin',\n",
    "                                     'clean_origin','origin_lat','origin_long',\n",
    "                                     'destination','country_destination','area_level_1_destination',\n",
    "                                     'clean_destination','destination_lat','destination_long',\n",
    "                                     'date', 'price', 'url','note']]\n",
    "separa_origin_df.to_csv(join(base_dir, \"separate_origins.csv\"), encoding='utf-8')\n",
    "print(len(separa_origin_df))\n",
    "separa_origin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separa_origin_df = pd.read_csv(join(base_dir, \"separate_origins.csv\"), index_col=0, encoding='utf-8')\n",
    "separa_origin_df.origin.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separa_origin_df[separa_origin_df.origin == 'CHI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
